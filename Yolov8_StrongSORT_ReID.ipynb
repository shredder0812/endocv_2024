{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQlmeNmhJSen"
      },
      "source": [
        "# Load video and model\n",
        "## Video:\n",
        "- video_test: short videos, 3 folders (HTT, UTTQ, UTDD)\n",
        "  - HTT: Duodenal -> use model htt.pt\n",
        "  - UTTQ: Esophageal cancer -> use model thucquan.pt\n",
        "  - UTDD: Gastric cancer - use model daday.pt\n",
        "- video_CS: IGH videos from Hoang Long Clinic\n",
        "  - HTT: Duodenal -> use model htt.pt\n",
        "  - UTTQ: Esophageal cancer -> use model thucquan.pt\n",
        "  - UTDD: Gastric cancer - use model daday.pt\n",
        "- data_pk: 5 videos (inflammatory object) -> can use (3 models) or 5-class model\n",
        "\n",
        "## YOLOv8 Model\n",
        "- 3 models and classes:\n",
        "  - htt.pt: 7_Loet_HTT\n",
        "  - thucquan.pt: 2_Viem_thuc_quan, 5_Ung_thu_thuc_quan\n",
        "  - daday.pt: 3_Viem_da_day_HP_am, 4_Viem_da_day_HP_duong, 6_Ung_thu_da_day\n",
        "- 1 model (5 classes): 5-class-model.pt\n",
        "  - 0: 2_Viem_thuc_quan\n",
        "  - 1: 3_Viem_da_day_HP_am\n",
        "  - 2: 5_Ung_thu_thuc_quan\n",
        "  - 3: 6_Ung_thu_da_day\n",
        "  - 4: 7_Loet_HTT\n",
        "\n",
        "## Re-ID Model\n",
        "- OSNet (Market1501-based): osnet_x0_25_endocv_30.pt\n",
        "\n",
        "## Usage\n",
        "- Mount Drive + Load model\n",
        "- Load Video data\n",
        "- Install requirements\n",
        "- Import libraries\n",
        "- File define (edit 'name=...' when change the video)\n",
        "- Class Color, StrongSORT\n",
        "- Class ObjectDetection (simple use)\n",
        "- RUN\n",
        "- Generate txt csv results\n",
        "\n",
        "The final results will be in folder \"/content/runs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UA2yotWAgjh"
      },
      "source": [
        "# Mount Drive + Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsZofyoUsGwP",
        "outputId": "7e59eaa6-fa97-4aeb-c9f9-b2d6b8ad8179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5IzdOAAWhVU"
      },
      "source": [
        "Load ReID model and 3 detection models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YlICV_DtpxX"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/torchreid_model/osnet_x0_25_endocv_30.pt /content/drive/MyDrive/ENDOCV/model_pt/model_yolo/daday.pt /content/drive/MyDrive/ENDOCV/model_pt/model_yolo/thucquan.pt /content/drive/MyDrive/ENDOCV/model_pt/model_yolo/htt.pt /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5ZhcP2fWpuS"
      },
      "source": [
        "Load ReID model and 5-class model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv3Ldz7ZWwmd"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/torchreid_model/osnet_x0_25_endocv_30.pt /content/drive/MyDrive/ENDOCV/model_pt/model_yolo/5-class-model.pt /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yViuoXlmy9N"
      },
      "source": [
        "# Load Video data (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A-ArEo7m1Sg"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/data_pk /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nru0aYhiBTUN"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/video_CS /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtYjyrYqF8Am"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/ENDOCV/video_test /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynwB96csJ2tf"
      },
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "6cYF-NnqFtN6",
        "outputId": "36ac9854-76e9-4f3d-9f43-a2579b4ff529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.38-py3-none-any.whl (792 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.5/792.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.2.38 ultralytics-thop-2.0.0\n",
            "Collecting boxmot\n",
            "  Downloading boxmot-10.0.73-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filterpy<2.0.0,>=1.4.5 (from boxmot)\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0.0,>=6.1.3 (from boxmot)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown<6.0.0,>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (5.1.0)\n",
            "Collecting gitpython<4.0.0,>=3.1.42 (from boxmot)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lapx<0.6.0,>=0.5.5 (from boxmot)\n",
            "  Downloading lapx-0.5.9.post1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru<0.8.0,>=0.7.2 (from boxmot)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.24.4 (from boxmot)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (4.8.0.76)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (2.0.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from boxmot) (6.0.1)\n",
            "Requirement already satisfied: regex<2025.0.0,>=2024.0.0 in /usr/local/lib/python3.10/dist-packages (from boxmot) (2024.5.15)\n",
            "Collecting scikit-learn<2.0.0,>=1.3.0 (from boxmot)\n",
            "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3.0.0,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from boxmot) (2.3.0+cu121)\n",
            "Collecting torchvision<0.18.0,>=0.17.1 (from boxmot)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yacs<0.2.0,>=0.1.8 (from boxmot)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy<2.0.0,>=1.4.5->boxmot) (1.11.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy<2.0.0,>=1.4.5->boxmot) (3.7.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0.0,>=6.1.3->boxmot) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown<6.0.0,>=5.1.0->boxmot) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown<6.0.0,>=5.1.0->boxmot) (3.15.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown<6.0.0,>=5.1.0->boxmot) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown<6.0.0,>=5.1.0->boxmot) (4.66.4)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4.0.0,>=3.1.42->boxmot)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.0->boxmot) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.0->boxmot) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.0->boxmot) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.3.0->boxmot) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.3.0->boxmot) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.2.1->boxmot) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.2.1->boxmot) (12.1.105)\n",
            "Collecting torch<3.0.0,>=2.2.1 (from boxmot)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.18.0,>=0.17.1->boxmot) (9.4.0)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch<3.0.0,>=2.2.1->boxmot)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.42->boxmot)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.0.0->boxmot) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown<6.0.0,>=5.1.0->boxmot) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.2.1->boxmot) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy<2.0.0,>=1.4.5->boxmot) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown<6.0.0,>=5.1.0->boxmot) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.2.1->boxmot) (1.3.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=24b19a32e3b0009bb6479dc7fd762007c03a733de5fa7d282c1f304cb7f0d430\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "Successfully built filterpy\n",
            "Installing collected packages: yacs, triton, smmap, nvidia-nccl-cu12, numpy, loguru, ftfy, lapx, gitdb, torch, scikit-learn, gitpython, torchvision, filterpy, boxmot\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.0+cu121\n",
            "    Uninstalling torchvision-0.18.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boxmot-10.0.73 filterpy-1.4.5 ftfy-6.2.0 gitdb-4.0.11 gitpython-3.1.43 lapx-0.5.9.post1 loguru-0.7.2 numpy-1.24.4 nvidia-nccl-cu12-2.19.3 scikit-learn-1.5.0 smmap-5.0.1 torch-2.2.2 torchvision-0.17.2 triton-2.2.0 yacs-0.1.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "614e2e751e7846be823ae7da4787491f",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install ultralytics\n",
        "!pip install boxmot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc58-XqGPcD7"
      },
      "source": [
        "# Import lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVCYa1d82CNQ",
        "outputId": "3f08ad3a-c8e7-4586-c9e9-827be1413486"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7fb2354b35b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n",
            "    self._make_controller_from_path(filepath)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 1175, in _make_controller_from_path\n",
            "    lib_controller = controller_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 114, in __init__\n",
            "    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-5007b62f.3.23.dev.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "from time import perf_counter, time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from boxmot import StrongSORT, BoTSORT, DeepOCSORT, OCSORT, HybridSORT\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGNN6uvKJVXF"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCnvPq5DZ2hH"
      },
      "source": [
        "# 1.1 File define (video_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdA687FiZ2hH",
        "outputId": "4d8a3aac-3d87-49e1-b5e9-869041adba91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['210518CS101', '230114CS2', '220324CS2', '210504CS205', '220702CS2'], ['220727CS201', '231103CS101', '231109CS101', '220318CS202', '231124CS101', '220922CS201'], []]\n"
          ]
        }
      ],
      "source": [
        "def get_files(directory):\n",
        "    files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        # Lấy tên file mà không có phần mở rộng\n",
        "        file_name_without_extension, _ = os.path.splitext(filename)\n",
        "        files.append(file_name_without_extension)\n",
        "    return files\n",
        "\n",
        "directory_A = \"/content/video_test/UTDD/\"\n",
        "directory_B = \"/content/video_test/UTTQ/\"\n",
        "directory_C = \"/content/video_test/HTT/\"\n",
        "\n",
        "files_A = get_files(directory_A)\n",
        "files_B = get_files(directory_B)\n",
        "files_C = get_files(directory_C)\n",
        "\n",
        "vid_utdd_uttq_htt = [files_A, files_B,files_C]\n",
        "\n",
        "print(vid_utdd_uttq_htt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEoDwSoMZ2hI"
      },
      "source": [
        "### video_test:\n",
        "- UTDD: ['220702CS2_Trim2', '210518CS101_Trim2', '220324CS2_Trim2', '230320BVK020_Trim2', '230114CS2_Trim', '230114CS2_Trim2', '210518CS101_Trim', '230320BVK020_Trim', '220702CS2_Trim', '220324CS2_Trim']\n",
        "- UTTQ: ['230411BVK107_Trim', '230411BVK106_Trim2', '220922CS201_Trim', '230411BVK004_Trim', '230411BVK004_Trim2', '220922CS201_Trim2', '230407BVK095_Trim2', '230411BVK104_Trim', '230411BVK106_Trim', '230407BVK095_Trim']\n",
        "- HTT: ['Da day 211207 CS1 02_Trim']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvKoQSHZ2hI",
        "outputId": "582d22fa-42d3-4fad-eebd-4d3a7532e5d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Video Name: UTTQ_231103CS101\n",
            "Model Classes: ['Viem thuc quan', 'Viem da day', 'Ung thu thuc quan', 'Ung thu da day', 'Loet HTT']\n"
          ]
        }
      ],
      "source": [
        "# Copy and paste video name\n",
        "name = '231103CS101'\n",
        "\n",
        "\n",
        "if name in vid_utdd_uttq_htt[0]:  # Kiểm tra xem name có trong chiều 1 không\n",
        "    test_vid = \"/content/video_test/UTDD/\" + name + \".mp4\"\n",
        "    model_weights = \"/content/daday.pt\"\n",
        "elif name in vid_utdd_uttq_htt[1]:\n",
        "    test_vid = \"/content/video_test/UTTQ/\" + name + \".mp4\"\n",
        "    model_weights = \"/content/thucquan.pt\"\n",
        "else:\n",
        "    test_vid = \"/content/video_test/HTT/\" + name + \".mp4\"\n",
        "    model_weights = \"/content/htt.pt\"\n",
        "\n",
        "\n",
        "input_video_name = test_vid.split(\"/\")[-2].split(\".\")[0] + '_' + test_vid.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "\n",
        "# Tạo từ điển ánh xạ giữa tên model_weights và model_classes\n",
        "model_classes_dict = {\n",
        "    \"/content/daday.pt\": ['Viem da day', 'Viem da day' , 'Ung thu da day'],\n",
        "    \"/content/thucquan.pt\": ['Viem thuc quan', 'Ung thu thuc quan'],\n",
        "    \"/content/htt.pt\": ['Loet HTT']\n",
        "}\n",
        "\n",
        "# Thiết lập model_classes từ từ điển, nếu không khớp thì trả về ['polyp', 'esophagael cancer']\n",
        "model_classes = model_classes_dict.get(model_weights, ['polyp', 'esophagael cancer'])\n",
        "\n",
        "\n",
        "print(\"Input Video Name:\", input_video_name)\n",
        "print(\"Model Classes:\", model_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuLvnDk_9rDw"
      },
      "source": [
        "# 1.2 File define (video_CS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IYLIqZBx1Gt",
        "outputId": "4d8a3aac-3d87-49e1-b5e9-869041adba91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['210518CS101', '230114CS2', '220324CS2', '210504CS205', '220702CS2'], ['220727CS201', '231103CS101', '231109CS101', '220318CS202', '231124CS101', '220922CS201'], []]\n"
          ]
        }
      ],
      "source": [
        "def get_files(directory):\n",
        "    files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        # Lấy tên file mà không có phần mở rộng\n",
        "        file_name_without_extension, _ = os.path.splitext(filename)\n",
        "        files.append(file_name_without_extension)\n",
        "    return files\n",
        "\n",
        "directory_A = \"/content/video_CS/UTDD/\"\n",
        "directory_B = \"/content/video_CS/UTTQ/\"\n",
        "directory_C = \"/content/video_CS/HTT/\"\n",
        "\n",
        "files_A = get_files(directory_A)\n",
        "files_B = get_files(directory_B)\n",
        "files_C = get_files(directory_C)\n",
        "\n",
        "vid_utdd_uttq_htt = [files_A, files_B,files_C]\n",
        "\n",
        "print(vid_utdd_uttq_htt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FIKScuoYuCv"
      },
      "source": [
        "### video_CS:\n",
        "- UTDD ['210504CS205', '220702CS2', '220324CS2', '210518CS101', '230114CS2'],\n",
        "- UTTQ ['231103CS101', '220318CS202', '231124CS101', '231109CS101', '220922CS201', '220727CS201']\n",
        "- HTT []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOgi7YhCC8uY",
        "outputId": "582d22fa-42d3-4fad-eebd-4d3a7532e5d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Video Name: UTTQ_231103CS101\n",
            "Model Classes: ['Viem thuc quan', 'Viem da day', 'Ung thu thuc quan', 'Ung thu da day', 'Loet HTT']\n"
          ]
        }
      ],
      "source": [
        "# Copy and paste video name\n",
        "name = '231103CS101'\n",
        "\n",
        "\n",
        "if name in vid_utdd_uttq_htt[0]:  # Kiểm tra xem name có trong chiều 1 không\n",
        "    test_vid = \"/content/video_test/UTDD/\" + name + \".mp4\"\n",
        "    model_weights = \"/content/daday.pt\"\n",
        "elif name in vid_utdd_uttq_htt[1]:\n",
        "    test_vid = \"/content/video_test/UTTQ/\" + name + \".mp4\"\n",
        "    model_weights = \"/content/thucquan.pt\"\n",
        "else:\n",
        "    test_vid = \"/content/video_test/HTT/\" + name + \".mp4\"\n",
        "    model_weights = \"/content/htt.pt\"\n",
        "\n",
        "input_video_name = test_vid.split(\"/\")[-2].split(\".\")[0] + '_' + test_vid.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "\n",
        "# Tạo từ điển ánh xạ giữa tên model_weights và model_classes\n",
        "model_classes_dict = {\n",
        "    \"/content/daday.pt\": ['Viem da day', 'Viem da day' , 'Ung thu da day'],\n",
        "    \"/content/thucquan.pt\": ['Viem thuc quan', 'Ung thu thuc quan'],\n",
        "    \"/content/htt.pt\": ['Loet HTT']\n",
        "\n",
        "}\n",
        "\n",
        "# Thiết lập model_classes từ từ điển, nếu không khớp thì trả về ['polyp', 'esophagael cancer']\n",
        "model_classes = model_classes_dict.get(model_weights, ['polyp', 'esophagael cancer'])\n",
        "\n",
        "\n",
        "print(\"Input Video Name:\", input_video_name)\n",
        "print(\"Model Classes:\", model_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVXaRpG4nLeX"
      },
      "source": [
        "# 1.3 File define (data_pk)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB3J82CwnLed",
        "outputId": "be52f730-7084-46ee-f4cf-f2d2abc121cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Da day 220111 CS1 05', 'Da day 200508 CS1 02', 'IGH AINN20 Tổng hợp timeframe video gui CNTT', 'Da day 200530 CS1 02', 'Da day 200512 CS1 01', 'Da day 200926 CS1 01']]\n"
          ]
        }
      ],
      "source": [
        "def get_files(directory):\n",
        "    files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        # Lấy tên file mà không có phần mở rộng\n",
        "        file_name_without_extension, _ = os.path.splitext(filename)\n",
        "        files.append(file_name_without_extension)\n",
        "    return files\n",
        "\n",
        "#directory_A = \"/content/video_CS/UTDD/\"\n",
        "directory_B = \"/content/data_pk/\"\n",
        "#directory_C = \"/content/video_CS/HTT/\"\n",
        "\n",
        "#files_A = get_files(directory_A)\n",
        "files_B = get_files(directory_B)\n",
        "#files_C = get_files(directory_C)\n",
        "\n",
        "vid_utdd_uttq_htt = [files_B]\n",
        "\n",
        "print(vid_utdd_uttq_htt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SczJWnGMnLee",
        "outputId": "2c395425-4676-43c0-a25d-cece5c13af51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Video Name: Da day 200512 CS1 01\n",
            "Model Classes: ['Viem da day', 'Viem da day', 'Ung thu da day']\n"
          ]
        }
      ],
      "source": [
        "# 'Da day 200926 CS1 01' DD TQ\n",
        "# 'Da day 200508 CS1 02' DD HTT TQ\n",
        "# 'Da day 200530 CS1 02' DD TQ HTT\n",
        "# 'Da day 200512 CS1 01' DD\n",
        "# 'Da day 220111 CS1 05' DD TQ HTT\n",
        "\n",
        "name = 'Da day 200512 CS1 01'\n",
        "\n",
        "if name in vid_utdd_uttq_htt[0]:  # Kiểm tra xem name có trong chiều 1 không\n",
        "    test_vid = \"/content/data_pk/\" + name + \".mp4\"\n",
        "    model_weights = \"/content/5-class-model.pt\n",
        "\"\n",
        "#input_video_name = test_vid.split(\"/\")[-2].split(\".\")[0] + '_' + test_vid.split(\"/\")[-1].split(\".\")[0]\n",
        "input_video_name = name\n",
        "\n",
        "\n",
        "# Tạo từ điển ánh xạ giữa tên model_weights và model_classes\n",
        "model_classes_dict = {\n",
        "    \"/content/5-class-model.pt\": ['Viem thuc quan', 'Viem da day' ,'Ung thu thuc quan', 'Ung thu da day', 'Loet HTT']\n",
        "}\n",
        "\n",
        "# Thiết lập model_classes từ từ điển, nếu không khớp thì trả về ['polyp', 'esophagael cancer']\n",
        "model_classes = model_classes_dict.get(model_weights, ['polyp', 'esophagael cancer'])\n",
        "\n",
        "\n",
        "print(\"Input Video Name:\", input_video_name)\n",
        "print(\"Model Classes:\", model_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIruipeA8jrt"
      },
      "source": [
        "# Class Color, Strongsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7yMlKTVKBMM"
      },
      "outputs": [],
      "source": [
        "class Colors:\n",
        "    def __init__(self, num_colors=80):\n",
        "        self.num_colors = num_colors\n",
        "        self.color_palette = self.generate_color_palette()\n",
        "\n",
        "\n",
        "    def generate_color_palette(self):\n",
        "        hsv_palette = np.zeros((self.num_colors, 1, 3), dtype=np.uint8)\n",
        "        hsv_palette[:, 0, 0] = np.linspace(0, 180, self.num_colors, endpoint=False)\n",
        "        hsv_palette[:, :, 1:] = 255\n",
        "        bgr_palette = cv2.cvtColor(hsv_palette, cv2.COLOR_HSV2BGR)\n",
        "        return bgr_palette.reshape(-1, 3)\n",
        "\n",
        "    def __call__(self, class_id):\n",
        "        color = tuple(map(int, self.color_palette[class_id]))\n",
        "        return color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR1h2yM2mBcb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from boxmot.appearance.reid_auto_backend import ReidAutoBackend\n",
        "from boxmot.motion.cmc import get_cmc_method\n",
        "from boxmot.trackers.strongsort.sort.detection import Detection\n",
        "from boxmot.trackers.strongsort.sort.tracker import Tracker\n",
        "from boxmot.utils.matching import NearestNeighborDistanceMetric\n",
        "from boxmot.utils.ops import xyxy2tlwh\n",
        "from boxmot.utils import PerClassDecorator\n",
        "\n",
        "\n",
        "class StrongSORT(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_weights,\n",
        "        device,\n",
        "        fp16,\n",
        "        per_class=False,\n",
        "        max_dist=0.2,\n",
        "        max_iou_dist=0.7,\n",
        "        max_age=30,\n",
        "        n_init=1,\n",
        "        nn_budget=100,\n",
        "        mc_lambda=0.995,\n",
        "        ema_alpha=0.9,\n",
        "    ):\n",
        "        self.max_dist=0.95,\n",
        "        self.max_iou_dist=0.95,\n",
        "        self.max_age=300,\n",
        "        self.per_class = per_class\n",
        "        rab = ReidAutoBackend(\n",
        "            weights=model_weights, device=device, half=fp16\n",
        "        )\n",
        "        self.model = rab.get_backend()\n",
        "        self.tracker = Tracker(\n",
        "            metric=NearestNeighborDistanceMetric(\"cosine\", max_dist, nn_budget),\n",
        "            max_iou_dist=max_iou_dist,\n",
        "            max_age=max_age,\n",
        "            n_init=n_init,\n",
        "            mc_lambda=mc_lambda,\n",
        "            ema_alpha=ema_alpha,\n",
        "        )\n",
        "        self.cmc = get_cmc_method('ecc')()\n",
        "\n",
        "    @PerClassDecorator\n",
        "    def update(self, dets, img, embs=None):\n",
        "        assert isinstance(\n",
        "            dets, np.ndarray\n",
        "        ), f\"Unsupported 'dets' input format '{type(dets)}', valid format is np.ndarray\"\n",
        "        assert isinstance(\n",
        "            img, np.ndarray\n",
        "        ), f\"Unsupported 'img' input format '{type(img)}', valid format is np.ndarray\"\n",
        "        assert (\n",
        "            len(dets.shape) == 2\n",
        "        ), \"Unsupported 'dets' dimensions, valid number of dimensions is two\"\n",
        "        assert (\n",
        "            dets.shape[1] == 6\n",
        "        ), \"Unsupported 'dets' 2nd dimension lenght, valid lenghts is 6\"\n",
        "\n",
        "        dets = np.hstack([dets, np.arange(len(dets)).reshape(-1, 1)])\n",
        "        xyxy = dets[:, 0:4]\n",
        "        confs = dets[:, 4]\n",
        "        clss = dets[:, 5]\n",
        "        det_ind = dets[:, 6]\n",
        "\n",
        "        if len(self.tracker.tracks) >= 1:\n",
        "            warp_matrix = self.cmc.apply(img, xyxy)\n",
        "            for track in self.tracker.tracks:\n",
        "                track.camera_update(warp_matrix)\n",
        "\n",
        "        # extract appearance information for each detection\n",
        "        if embs is not None:\n",
        "            features = embs\n",
        "        else:\n",
        "            features = self.model.get_features(xyxy, img)\n",
        "\n",
        "        tlwh = xyxy2tlwh(xyxy)\n",
        "        detections = [\n",
        "            Detection(box, conf, cls, det_ind, feat) for\n",
        "            box, conf, cls, det_ind, feat in\n",
        "            zip(tlwh, confs, clss, det_ind, features)\n",
        "        ]\n",
        "\n",
        "        # update tracker\n",
        "        self.tracker.predict()\n",
        "        self.tracker.update(detections)\n",
        "\n",
        "        # output bbox identities\n",
        "        outputs = []\n",
        "        for track in self.tracker.tracks:\n",
        "            if not track.is_confirmed() or track.time_since_update >= 1:\n",
        "                continue\n",
        "\n",
        "            x1, y1, x2, y2 = track.to_tlbr()\n",
        "\n",
        "            id = track.id\n",
        "            conf = track.conf\n",
        "            cls = track.cls\n",
        "            det_ind = track.det_ind\n",
        "\n",
        "            outputs.append(\n",
        "                np.concatenate(([x1, y1, x2, y2], [id], [conf], [cls], [det_ind])).reshape(1, -1)\n",
        "            )\n",
        "        if len(outputs) > 0:\n",
        "            return np.concatenate(outputs)\n",
        "        return np.array([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-OOpopN4h0z"
      },
      "source": [
        "# 2. Class ObjectDetection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej2L0QCj0Unl"
      },
      "source": [
        "## Simple use (recommend)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI11Qbtkwkx7"
      },
      "outputs": [],
      "source": [
        "class ObjectDetection:\n",
        "    def __init__(self, model_weights=\"yolov8s.pt\", capture_index=0, min_temporal_threshold=0, max_temporal_threshold=0, iou_threshold=0.2, use_frame_id=False):\n",
        "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "        print(\"Using Device: \", self.device)\n",
        "        self.model = self.load_model(model_weights)\n",
        "        self.classes = self.model.names\n",
        "        self.classes = model_classes\n",
        "        self.colors = Colors(len(self.classes))\n",
        "        self.font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        self.capture_index = capture_index\n",
        "        self.cap = self.load_capture()\n",
        "        reid_weights = Path(\"/content/osnet_x0_25_endocv_30.pt\")\n",
        "        self.tracker = StrongSORT(reid_weights,\n",
        "                                  torch.device(self.device),\n",
        "                                  fp16 = False,\n",
        "                                  max_dist=0.95,\n",
        "                                  max_iou_dist=0.95,\n",
        "                                  max_age=300\n",
        "                                  )\n",
        "        self.min_temporal_threshold = min_temporal_threshold\n",
        "        self.max_temporal_threshold = max_temporal_threshold\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.use_frame_id = use_frame_id\n",
        "\n",
        "    def load_model(self, weights):\n",
        "        model = YOLO(weights)\n",
        "        model.fuse()\n",
        "        return model\n",
        "\n",
        "    def predict(self, frame):\n",
        "        results = self.model(frame, stream=True, verbose=False, conf=0.6, line_width=1)\n",
        "        return results\n",
        "\n",
        "    def _frame_idx_to_hmsf(self, frame_id: int):\n",
        "        \"\"\"convert to hmsf timestamp by given frame idx and fps\"\"\"\n",
        "        self.video_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        assert self.video_fps\n",
        "        base = datetime.strptime('00:00:00.000000', '%H:%M:%S.%f')\n",
        "        delta = timedelta(seconds=frame_id/self.video_fps)\n",
        "        return (base + delta).strftime('%H:%M:%S.%f')\n",
        "\n",
        "    def _frame_idx_to_hms(self, frame_id: int):\n",
        "        \"\"\"convert to hms timestamp by given frame idx and fps\"\"\"\n",
        "        self.video_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        assert self.video_fps\n",
        "        base = datetime.strptime('00:00:00', '%H:%M:%S')\n",
        "        delta = timedelta(seconds=frame_id//self.video_fps)\n",
        "        return (base + delta).strftime('%H:%M:%S')\n",
        "\n",
        "    def draw_tracks(self, frame, tracks, txt_file, overlap_threshold=0.5):\n",
        "        seq_length = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_rate = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        im_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        im_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        frame_id = int(self.cap.get(cv2.CAP_PROP_POS_FRAMES))-1\n",
        "        timestamp_hms = self._frame_idx_to_hms(frame_id)\n",
        "        timestamp_hmsf = self._frame_idx_to_hmsf(frame_id)\n",
        "        null_notes = \"Tracking\"\n",
        "        for track in tracks:\n",
        "            x1, y1, x2, y2 = int(track[0]), int(track[1]), int(track[2]), int(track[3])\n",
        "            id = int(track[4])\n",
        "            conf = round(track[5], 2)\n",
        "            class_id = int(track[6])\n",
        "            class_name = self.classes[class_id]\n",
        "            cv2.rectangle(frame, (x1,y1), (x2, y2), self.colors(class_id), 5)\n",
        "            label = f'{class_name}, ID: {id}' # hiển thị\n",
        "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 5)\n",
        "            cv2.rectangle(frame, (x1, y1+h+15), (x1+w, y1), self.colors(class_id), -1)\n",
        "            cv2.putText(frame, label, (x1,y1+h+10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255) , 3)\n",
        "            # Ghi kết quả vào file txt\n",
        "            center_x = (x1 + x2) / 2\n",
        "            center_y = (y1 + y2) / 2\n",
        "            scale_height = frame.shape[0]\n",
        "            scale_width = frame.shape[1]\n",
        "            txt_file.write(f\"{timestamp_hms},{timestamp_hmsf},{frame_id},{frame_rate},{class_name},{id},{id},{null_notes},{frame.shape[0]},{frame.shape[1]},{scale_height},{scale_width},{x1},{y1},{x2},{y2},{center_x},{center_y}\\n\")\n",
        "            #txt_file.write(f\"{int(frame_id)},{id},{x1},{y1},{x2-x1},{y2-y1},{conf},-1,-1,-1\\n\")\n",
        "\n",
        "        return frame\n",
        "\n",
        "\n",
        "    def load_capture(self):\n",
        "        cap = cv2.VideoCapture(self.capture_index)\n",
        "        assert cap.isOpened()\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
        "        video_name = \"strongsort_\" + input_video_name + \".mp4\"\n",
        "        self.writer = cv2.VideoWriter(video_name\n",
        "        , cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "        return cap\n",
        "\n",
        "    def write_seqinfo_ini(self, seq_name, seq_length, frame_rate, im_width, im_height, im_ext, im_dir):\n",
        "        with open(\"seqinfo.ini\", \"w\") as f:\n",
        "            f.write(\"[Sequence]\\n\")\n",
        "            f.write(f\"name={seq_name}\\n\")\n",
        "            f.write(f\"imDir={im_dir}\\n\")  # Thay thế bằng thư mục chứa ảnh nếu cần\n",
        "            f.write(f\"frameRate={frame_rate}\\n\")\n",
        "            f.write(f\"seqLength={seq_length}\\n\")\n",
        "            f.write(f\"imWidth={im_width}\\n\")\n",
        "            f.write(f\"imHeight={im_height}\\n\")\n",
        "            f.write(f\"imExt={im_ext}\\n\")\n",
        "\n",
        "    def calculate_iou(self, box1, box2):\n",
        "        \"\"\"\n",
        "        Calculate intersection over union (IoU) between two bounding boxes.\n",
        "\n",
        "        Parameters:\n",
        "        - box1 (list): [x1, y1, x2, y2] of the first box.\n",
        "        - box2 (list): [x1, y1, x2, y2] of the second box.\n",
        "\n",
        "        Returns:\n",
        "        - iou (float): Intersection over Union (IoU) value.\n",
        "        \"\"\"\n",
        "        # Calculate intersection area\n",
        "        x1 = max(box1[0], box2[0])\n",
        "        y1 = max(box1[1], box2[1])\n",
        "        x2 = min(box1[2], box2[2])\n",
        "        y2 = min(box1[3], box2[3])\n",
        "        intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
        "\n",
        "        # Calculate areas of each bounding box\n",
        "        box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "        box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        "\n",
        "        # Calculate union area\n",
        "        union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "        # Calculate IoU\n",
        "        iou = intersection_area / union_area\n",
        "\n",
        "        return iou\n",
        "\n",
        "    def update_track_id(self, current_tracks, previous_tracks):\n",
        "        updated_tracks = []\n",
        "        for current_track in current_tracks:\n",
        "            min_distance = float('inf')\n",
        "            matching_track_id = None\n",
        "            for previous_track in previous_tracks:\n",
        "                if current_track[6] != previous_track[6]:\n",
        "                    continue  # Skip tracks of different classes\n",
        "                iou = self.calculate_iou(current_track[:4], previous_track[:4])\n",
        "                #print(iou, self.iou_threshold)\n",
        "                if iou > self.iou_threshold:\n",
        "                    if self.use_frame_id:\n",
        "                        time_diff = abs(current_track[3] - previous_track[3])\n",
        "                        if time_diff < min_distance:\n",
        "                            min_distance = time_diff\n",
        "                            matching_track_id = previous_track[4]\n",
        "                    else:\n",
        "                        time_diff = abs(current_track[1] - previous_track[1])\n",
        "                        if time_diff < min_distance:\n",
        "                            min_distance = time_diff\n",
        "                            matching_track_id = previous_track[4]\n",
        "\n",
        "            if matching_track_id is not None:\n",
        "                current_track[4] = matching_track_id\n",
        "            updated_tracks.append(current_track)\n",
        "        return updated_tracks\n",
        "\n",
        "    def __call__(self):\n",
        "        tracker = self.tracker\n",
        "\n",
        "        # Lấy thông tin từ video kết quả\n",
        "        seq_name = \"StrongSort\"\n",
        "        im_dir = \"img1\"\n",
        "        seq_length = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_rate = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        im_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        im_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        im_ext = \".jpg\"  # Phần mở rộng của ảnh\n",
        "\n",
        "        # Ghi thông tin vào file seqinfo.ini\n",
        "        self.write_seqinfo_ini(seq_name, seq_length, frame_rate, im_width, im_height, im_ext, im_dir)\n",
        "\n",
        "        # Mở file txt để ghi kết quả\n",
        "        with open(\"tracking_result.txt\", \"w\") as txt_file:\n",
        "            txt_file.write(\"timestamp_hms,timestamp_hmsf,frame_idx,fps,object_cls,object_idx,object_id,notes,frame_height,frame_width,scale_height,scale_width,x1,y1,x2,y2,center_x,center_y\\n\")\n",
        "            previous_tracks = []\n",
        "            while True:\n",
        "                start_time = perf_counter()\n",
        "                ret, frame = self.cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                cv2.rectangle(frame, (0, 30), (220, 80), (255, 255, 255), -1)\n",
        "                detections = self.predict(frame)\n",
        "                for dets in detections:\n",
        "                    tracks = tracker.update(dets.boxes.data.to(\"cpu\").numpy(), frame)\n",
        "                    if len(tracks.shape) == 2 and tracks.shape[1] == 8:\n",
        "                        if len(previous_tracks) > 0:\n",
        "                            tracks = self.update_track_id(tracks, previous_tracks)\n",
        "                        frame = self.draw_tracks(frame, tracks, txt_file)\n",
        "                        previous_tracks = tracks\n",
        "\n",
        "                end_time = perf_counter()\n",
        "                fps = 1 / np.round(end_time - start_time, 2)\n",
        "                cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), self.font, 1.5, (0, 255, 0), 5)\n",
        "                self.writer.write(frame)\n",
        "                # cv2_imshow(frame)\n",
        "                if cv2.waitKey(5) & 0xFF == 27:\n",
        "                    break\n",
        "            self.cap.release()\n",
        "            self.writer.release()\n",
        "            cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp-YuAoGXA-X"
      },
      "source": [
        "## Nhãn bên trái (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzdgLLjvWvK9"
      },
      "outputs": [],
      "source": [
        "class ObjectDetection:\n",
        "    def __init__(self, model_weights=\"yolov8s.pt\", capture_index=0, min_temporal_threshold=0, max_temporal_threshold=0, iou_threshold=0.2, use_frame_id=False):\n",
        "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "        print(\"Using Device: \", self.device)\n",
        "        self.model = self.load_model(model_weights)\n",
        "        self.classes = self.model.names\n",
        "        self.classes = model_classes\n",
        "        self.colors = Colors(len(self.classes))\n",
        "        self.font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        self.capture_index = capture_index\n",
        "        self.cap = self.load_capture()\n",
        "        reid_weights = Path(\"/content/osnet_x0_25_endocv_30.pt\")\n",
        "        self.tracker = StrongSORT(reid_weights,\n",
        "                                  torch.device(self.device),\n",
        "                                  fp16 = False,\n",
        "                                  max_dist=0.95,\n",
        "                                  max_iou_dist=0.95,\n",
        "                                  max_age=300\n",
        "                                  )\n",
        "        self.min_temporal_threshold = min_temporal_threshold\n",
        "        self.max_temporal_threshold = max_temporal_threshold\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.use_frame_id = use_frame_id\n",
        "        self.labels = {}\n",
        "        self.saved_images = {}\n",
        "        self.last_detected_frame = None\n",
        "\n",
        "    def load_model(self, weights):\n",
        "        model = YOLO(weights)\n",
        "        model.fuse()\n",
        "        return model\n",
        "\n",
        "    def predict(self, frame):\n",
        "        results = self.model(frame, stream=True, verbose=False, conf=0.6, line_width=1)\n",
        "        return results\n",
        "\n",
        "    def _frame_idx_to_hmsf(self, frame_id: int):\n",
        "        \"\"\"convert to hmsf timestamp by given frame idx and fps\"\"\"\n",
        "        self.video_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        assert self.video_fps\n",
        "        base = datetime.strptime('00:00:00.000000', '%H:%M:%S.%f')\n",
        "        delta = timedelta(seconds=frame_id/self.video_fps)\n",
        "        return (base + delta).strftime('%H:%M:%S.%f')\n",
        "\n",
        "    def _frame_idx_to_hms(self, frame_id: int):\n",
        "        \"\"\"convert to hms timestamp by given frame idx and fps\"\"\"\n",
        "        self.video_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        assert self.video_fps\n",
        "        base = datetime.strptime('00:00:00', '%H:%M:%S')\n",
        "        delta = timedelta(seconds=frame_id//self.video_fps)\n",
        "        return (base + delta).strftime('%H:%M:%S')\n",
        "\n",
        "    def load_capture(self):\n",
        "        cap = cv2.VideoCapture(self.capture_index)\n",
        "        assert cap.isOpened()\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
        "        video_name = \"strongsort_\" + input_video_name + \".mp4\"\n",
        "        self.writer = cv2.VideoWriter(video_name\n",
        "        , cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "        return cap\n",
        "\n",
        "    def write_seqinfo_ini(self, seq_name, seq_length, frame_rate, im_width, im_height, im_ext, im_dir):\n",
        "        with open(\"seqinfo.ini\", \"w\") as f:\n",
        "            f.write(\"[Sequence]\\n\")\n",
        "            f.write(f\"name={seq_name}\\n\")\n",
        "            f.write(f\"imDir={im_dir}\\n\")  # Thay thế bằng thư mục chứa ảnh nếu cần\n",
        "            f.write(f\"frameRate={frame_rate}\\n\")\n",
        "            f.write(f\"seqLength={seq_length}\\n\")\n",
        "            f.write(f\"imWidth={im_width}\\n\")\n",
        "            f.write(f\"imHeight={im_height}\\n\")\n",
        "            f.write(f\"imExt={im_ext}\\n\")\n",
        "\n",
        "    def calculate_iou(self, box1, box2):\n",
        "        \"\"\"\n",
        "        Calculate intersection over union (IoU) between two bounding boxes.\n",
        "\n",
        "        Parameters:\n",
        "        - box1 (list): [x1, y1, x2, y2] of the first box.\n",
        "        - box2 (list): [x1, y1, x2, y2] of the second box.\n",
        "\n",
        "        Returns:\n",
        "        - iou (float): Intersection over Union (IoU) value.\n",
        "        \"\"\"\n",
        "        # Calculate intersection area\n",
        "        x1 = max(box1[0], box2[0])\n",
        "        y1 = max(box1[1], box2[1])\n",
        "        x2 = min(box1[2], box2[2])\n",
        "        y2 = min(box1[3], box2[3])\n",
        "        intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
        "\n",
        "        # Calculate areas of each bounding box\n",
        "        box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "        box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        "\n",
        "        # Calculate union area\n",
        "        union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "        # Calculate IoU\n",
        "        iou = intersection_area / union_area\n",
        "\n",
        "        return iou\n",
        "\n",
        "    def update_track_id(self, current_tracks, previous_tracks):\n",
        "        updated_tracks = []\n",
        "        for current_track in current_tracks:\n",
        "            min_distance = float('inf')\n",
        "            matching_track_id = None\n",
        "            for previous_track in previous_tracks:\n",
        "                if current_track[6] != previous_track[6]:\n",
        "                    continue  # Skip tracks of different classes\n",
        "                iou = self.calculate_iou(current_track[:4], previous_track[:4])\n",
        "                #print(iou, self.iou_threshold)\n",
        "                if iou > self.iou_threshold:\n",
        "                    if self.use_frame_id:\n",
        "                        time_diff = abs(current_track[3] - previous_track[3])\n",
        "                        if time_diff < min_distance:\n",
        "                            min_distance = time_diff\n",
        "                            matching_track_id = previous_track[4]\n",
        "                    else:\n",
        "                        time_diff = abs(current_track[1] - previous_track[1])\n",
        "                        if time_diff < min_distance:\n",
        "                            min_distance = time_diff\n",
        "                            matching_track_id = previous_track[4]\n",
        "\n",
        "            if matching_track_id is not None:\n",
        "                current_track[4] = matching_track_id\n",
        "            updated_tracks.append(current_track)\n",
        "        return updated_tracks\n",
        "\n",
        "    def draw_tracks(self, frame, tracks, txt_file, overlap_threshold=0.5):\n",
        "        seq_length = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_rate = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        im_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        im_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        frame_id = int(self.cap.get(cv2.CAP_PROP_POS_FRAMES))-1\n",
        "        timestamp_hms = self._frame_idx_to_hms(frame_id)\n",
        "        timestamp_hmsf = self._frame_idx_to_hmsf(frame_id)\n",
        "        null_notes = \"Tracking\"\n",
        "        for track in tracks:\n",
        "            x1, y1, x2, y2 = int(track[0]), int(track[1]), int(track[2]), int(track[3])\n",
        "            id = int(track[4])\n",
        "            conf = round(track[5], 2)\n",
        "            class_id = int(track[6])\n",
        "            class_name = self.classes[class_id]\n",
        "            cv2.rectangle(frame, (x1,y1), (x2, y2), self.colors(class_id), 5)\n",
        "            self.save_first_detected_frame(frame, track)\n",
        "            # Update label if the object ID is new or changed\n",
        "            if id not in self.labels:\n",
        "                self.labels[id] = class_name\n",
        "\n",
        "            # Write result to txt file\n",
        "            center_x = (x1 + x2) / 2\n",
        "            center_y = (y1 + y2) / 2\n",
        "            scale_height = frame.shape[0]\n",
        "            scale_width = frame.shape[1]\n",
        "            txt_file.write(f\"{timestamp_hms},{timestamp_hmsf},{frame_id},{frame_rate},{class_name},{id},{id},{null_notes},{frame.shape[0]},{frame.shape[1]},{scale_height},{scale_width},{x1},{y1},{x2},{y2},{center_x},{center_y}\\n\")\n",
        "            #detected_ids.add(id)\n",
        "        return frame\n",
        "\n",
        "    def display_labels(self, frame, tracks):\n",
        "        # Tạo một từ điển để lưu trữ các nhãn đã được gán\n",
        "        labels_dict = {}\n",
        "\n",
        "        # Lặp qua các tracks và cập nhật từ điển labels_dict\n",
        "        for track in tracks:\n",
        "            id = int(track[4])\n",
        "            class_id = int(track[6])\n",
        "            class_name = self.classes[class_id]\n",
        "            labels_dict[id] = class_name\n",
        "\n",
        "        # Biến lưu màu của nhãn trước đó\n",
        "        previous_label_colors = {}\n",
        "\n",
        "        # Hiển thị nhãn trên khung hình\n",
        "        for id, label in self.labels.items():\n",
        "            label = f'{self.labels[id]}, ID: {id}'\n",
        "            if id in labels_dict:\n",
        "                # Nếu đối tượng có trong tracks, hiển thị nhãn mới\n",
        "                self.labels[id] = labels_dict[id]\n",
        "                class_id = int(track[6])\n",
        "                label_color = (0, 255, 0)\n",
        "                previous_label_colors[id] = label_color  # Lưu màu của nhãn mới\n",
        "            else:\n",
        "                # Nếu không phát hiện được đối tượng trong frame, sử dụng màu của nhãn trước đó\n",
        "                label_color = previous_label_colors.get(id, (0, 0, 255))\n",
        "            # Hiển thị nhãn trên khung hình\n",
        "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 5)\n",
        "            label_x = 0\n",
        "            label_y = 50 + h\n",
        "            cv2.rectangle(frame, (label_x, label_y - h - 15), (label_x + w + 10,label_y + 10), (0, 0, 0), -1)\n",
        "            cv2.putText(frame, label, (label_x + 5, label_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.5, label_color, 3)\n",
        "\n",
        "        return frame\n",
        "    def save_first_detected_frame(self, frame, track):\n",
        "        x1, y1, x2, y2 = int(track[0]), int(track[1]), int(track[2]), int(track[3])\n",
        "        id = int(track[4])\n",
        "        class_id = int(track[6])\n",
        "        key = (id, class_id)\n",
        "        if key not in self.saved_images:\n",
        "            object_img = frame[y1:y2, x1:x2]\n",
        "            height, width = object_img.shape[:2]\n",
        "            aspect_ratio = width / height\n",
        "            new_width = 300\n",
        "            new_height = int(new_width / aspect_ratio)\n",
        "            resized_img = cv2.resize(object_img, (new_width, new_height))\n",
        "            self.saved_images[key] = resized_img\n",
        "\n",
        "    def draw_saved_images(self, frame):\n",
        "        for (id, class_id), img in self.saved_images.items():\n",
        "            x_offset = 20\n",
        "            y_offset = 100\n",
        "            y_end = y_offset + img.shape[0]\n",
        "            x_end = x_offset + img.shape[1]\n",
        "            frame[y_offset:y_end, x_offset:x_end] = img\n",
        "        return frame\n",
        "\n",
        "\n",
        "    def __call__(self):\n",
        "        tracker = self.tracker\n",
        "\n",
        "        # Lấy thông tin từ video kết quả\n",
        "        seq_name = \"StrongSort\"\n",
        "        im_dir = \"img1\"\n",
        "        seq_length = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_rate = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        im_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        im_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        im_ext = \".jpg\"  # Phần mở rộng của ảnh\n",
        "\n",
        "        # Ghi thông tin vào file seqinfo.ini\n",
        "        self.write_seqinfo_ini(seq_name, seq_length, frame_rate, im_width, im_height, im_ext, im_dir)\n",
        "\n",
        "        # Mở file txt để ghi kết quả\n",
        "        with open(\"tracking_result.txt\", \"w\") as txt_file:\n",
        "            txt_file.write(\"timestamp_hms,timestamp_hmsf,frame_idx,fps,object_cls,object_idx,object_id,notes,frame_height,frame_width,scale_height,scale_width,x1,y1,x2,y2,center_x,center_y\\n\")\n",
        "            previous_tracks = []\n",
        "            while True:\n",
        "                start_time = perf_counter()\n",
        "                ret, frame = self.cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                label = \"Unknown\"\n",
        "                (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 5)\n",
        "                label_x = 0\n",
        "                label_y = 50 + h\n",
        "                cv2.rectangle(frame, (label_x, label_y - h - 15), (label_x + w + 10, label_y + 10), (0, 0, 0), -1)\n",
        "                cv2.putText(frame, label, (label_x + 5, label_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 3)\n",
        "\n",
        "                detections = self.predict(frame)\n",
        "                for dets in detections:\n",
        "                    tracks = tracker.update(dets.boxes.data.to(\"cpu\").numpy(), frame)\n",
        "                    if len(tracks.shape) == 2 and tracks.shape[1] == 8:\n",
        "                        if len(previous_tracks) > 0:\n",
        "                            tracks = self.update_track_id(tracks, previous_tracks)\n",
        "                        frame = self.draw_tracks(frame, tracks, txt_file)\n",
        "                        previous_tracks = tracks\n",
        "                self.display_labels(frame, tracks)\n",
        "                self.draw_saved_images(frame)\n",
        "                end_time = perf_counter()\n",
        "                # fps = 1 / np.round(end_time - start_time, 2)\n",
        "                # cv2.rectangle(frame, (0, 30), (220, 80), (255, 255, 255), -1)\n",
        "                # cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), self.font, 1.5, (0, 255, 0), 5)\n",
        "                self.writer.write(frame)\n",
        "                # cv2_imshow(frame)\n",
        "                if cv2.waitKey(5) & 0xFF == 27:\n",
        "                    break\n",
        "            self.cap.release()\n",
        "            self.writer.release()\n",
        "            cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sMsJnidXEhq"
      },
      "source": [
        "## Nhãn bên phải (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYbAzIJbXXsX"
      },
      "outputs": [],
      "source": [
        "class ObjectDetection:\n",
        "    def __init__(self, model_weights=\"yolov8s.pt\", capture_index=0, min_temporal_threshold=0, max_temporal_threshold=0, iou_threshold=0.2, use_frame_id=False):\n",
        "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "        print(\"Using Device: \", self.device)\n",
        "        self.model = self.load_model(model_weights)\n",
        "        self.classes = self.model.names\n",
        "        self.classes = model_classes\n",
        "        self.colors = Colors(len(self.classes))\n",
        "        self.font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        self.capture_index = capture_index\n",
        "        self.cap = self.load_capture()\n",
        "        reid_weights = Path(\"/content/osnet_x0_25_endocv_30.pt\")\n",
        "        self.tracker = StrongSORT(reid_weights,\n",
        "                                  torch.device(self.device),\n",
        "                                  fp16 = False,\n",
        "                                  max_dist=0.95,\n",
        "                                  max_iou_dist=0.95,\n",
        "                                  max_age=300\n",
        "                                  )\n",
        "        self.min_temporal_threshold = min_temporal_threshold\n",
        "        self.max_temporal_threshold = max_temporal_threshold\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.use_frame_id = use_frame_id\n",
        "        self.labels = {}\n",
        "        self.saved_images = {}\n",
        "        self.last_detected_frame = None\n",
        "\n",
        "    def load_model(self, weights):\n",
        "        model = YOLO(weights)\n",
        "        model.fuse()\n",
        "        return model\n",
        "\n",
        "    def predict(self, frame):\n",
        "        results = self.model(frame, stream=True, verbose=False, conf=0.5, line_width=1)\n",
        "        return results\n",
        "\n",
        "    def _frame_idx_to_hmsf(self, frame_id: int):\n",
        "        \"\"\"convert to hmsf timestamp by given frame idx and fps\"\"\"\n",
        "        self.video_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        assert self.video_fps\n",
        "        base = datetime.strptime('00:00:00.000000', '%H:%M:%S.%f')\n",
        "        delta = timedelta(seconds=frame_id/self.video_fps)\n",
        "        return (base + delta).strftime('%H:%M:%S.%f')\n",
        "\n",
        "    def _frame_idx_to_hms(self, frame_id: int):\n",
        "        \"\"\"convert to hms timestamp by given frame idx and fps\"\"\"\n",
        "        self.video_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        assert self.video_fps\n",
        "        base = datetime.strptime('00:00:00', '%H:%M:%S')\n",
        "        delta = timedelta(seconds=frame_id//self.video_fps)\n",
        "        return (base + delta).strftime('%H:%M:%S')\n",
        "\n",
        "    def load_capture(self):\n",
        "        cap = cv2.VideoCapture(self.capture_index)\n",
        "        assert cap.isOpened()\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
        "        video_name = \"tracking_\" + input_video_name + \".mp4\"\n",
        "        self.writer = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "        return cap\n",
        "\n",
        "    def write_seqinfo_ini(self, seq_name, seq_length, frame_rate, im_width, im_height, im_ext, im_dir):\n",
        "        with open(\"seqinfo.ini\", \"w\") as f:\n",
        "            f.write(\"[Sequence]\\n\")\n",
        "            f.write(f\"name={seq_name}\\n\")\n",
        "            f.write(f\"imDir={im_dir}\\n\")  # Thay thế bằng thư mục chứa ảnh nếu cần\n",
        "            f.write(f\"frameRate={frame_rate}\\n\")\n",
        "            f.write(f\"seqLength={seq_length}\\n\")\n",
        "            f.write(f\"imWidth={im_width}\\n\")\n",
        "            f.write(f\"imHeight={im_height}\\n\")\n",
        "            f.write(f\"imExt={im_ext}\\n\")\n",
        "\n",
        "    def calculate_iou(self, box1, box2):\n",
        "        # Calculate intersection area\n",
        "        x1 = max(box1[0], box2[0])\n",
        "        y1 = max(box1[1], box2[1])\n",
        "        x2 = min(box1[2], box2[2])\n",
        "        y2 = min(box1[3], box2[3])\n",
        "        intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
        "\n",
        "        # Calculate areas of each bounding box\n",
        "        box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "        box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        "\n",
        "        # Calculate union area\n",
        "        union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "        # Calculate IoU\n",
        "        iou = intersection_area / union_area\n",
        "\n",
        "        return iou\n",
        "\n",
        "    def update_track_id(self, current_tracks, previous_tracks):\n",
        "        updated_tracks = []\n",
        "        for current_track in current_tracks:\n",
        "            min_distance = float('inf')\n",
        "            matching_track_id = None\n",
        "            for previous_track in previous_tracks:\n",
        "                if current_track[6] != previous_track[6]:\n",
        "                    continue  # Skip tracks of different classes\n",
        "                iou = self.calculate_iou(current_track[:4], previous_track[:4])\n",
        "                #print(iou, self.iou_threshold)\n",
        "                if iou > self.iou_threshold:\n",
        "                    if self.use_frame_id:\n",
        "                        time_diff = abs(current_track[3] - previous_track[3])\n",
        "                        if time_diff < min_distance:\n",
        "                            min_distance = time_diff\n",
        "                            matching_track_id = previous_track[4]\n",
        "                    else:\n",
        "                        time_diff = abs(current_track[1] - previous_track[1])\n",
        "                        if time_diff < min_distance:\n",
        "                            min_distance = time_diff\n",
        "                            matching_track_id = previous_track[4]\n",
        "\n",
        "            if matching_track_id is not None:\n",
        "                current_track[4] = matching_track_id\n",
        "            updated_tracks.append(current_track)\n",
        "        return updated_tracks\n",
        "\n",
        "    def draw_tracks(self, frame, tracks, txt_file, overlap_threshold=0.5):\n",
        "        seq_length = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_rate = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        im_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        im_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        frame_id = int(self.cap.get(cv2.CAP_PROP_POS_FRAMES))-1\n",
        "        timestamp_hms = self._frame_idx_to_hms(frame_id)\n",
        "        timestamp_hmsf = self._frame_idx_to_hmsf(frame_id)\n",
        "        null_notes = \"Tracking\"\n",
        "        labels_dict = {}\n",
        "        for track in tracks:\n",
        "            x1, y1, x2, y2 = int(track[0]), int(track[1]), int(track[2]), int(track[3])\n",
        "            id = int(track[4])\n",
        "            conf = round(track[5], 2)\n",
        "            class_id = int(track[6])\n",
        "            class_name = self.classes[class_id]\n",
        "            cv2.rectangle(frame, (x1,y1), (x2, y2), self.colors(class_id), 5)\n",
        "            # self.save_first_detected_frame(frame, track)\n",
        "            # Write result to txt file\n",
        "            center_x = (x1 + x2) / 2\n",
        "            center_y = (y1 + y2) / 2\n",
        "            scale_height = frame.shape[0]\n",
        "            scale_width = frame.shape[1]\n",
        "            # Update label if the object ID is new or changed\n",
        "            if id not in self.labels:\n",
        "                self.labels[id] = class_name\n",
        "            self.save_first_detected_frame(frame, track)\n",
        "            txt_file.write(f\"{timestamp_hms},{timestamp_hmsf},{frame_id},{frame_rate},{class_name},{id},{id},{null_notes},{frame.shape[0]},{frame.shape[1]},{scale_height},{scale_width},{x1},{y1},{x2},{y2},{center_x},{center_y}\\n\")\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def display_labels(self, frame, tracks):\n",
        "        # Tạo một từ điển để lưu trữ các nhãn đã được gán\n",
        "        frame_id = int(self.cap.get(cv2.CAP_PROP_POS_FRAMES))-1\n",
        "        labels_dict = {}\n",
        "        last_detection_times = {}\n",
        "        previous_label_colors = {}\n",
        "\n",
        "        # Lặp qua các tracks và cập nhật từ điển labels_dict\n",
        "        for track in tracks:\n",
        "            id = int(track[4])\n",
        "            class_id = int(track[6])\n",
        "            class_name = self.classes[class_id]\n",
        "            labels_dict[id] = class_name\n",
        "        # Hiển thị nhãn trên khung hình\n",
        "        for id, label in self.labels.items():\n",
        "            # label = f'{self.labels[id]}, ID: {id}'\n",
        "            if id in labels_dict:\n",
        "                # Nếu đối tượng có trong tracks, hiển thị nhãn mới\n",
        "                self.labels[id] = labels_dict[id]\n",
        "                class_id = int(track[6])\n",
        "                label_color = self.colors(class_id)\n",
        "                previous_label_colors[id] = label_color\n",
        "                last_detection_times[id] = time()  # Lưu màu của nhãn mới\n",
        "                label = f'{self.labels[id]}, ID: {id}'\n",
        "            else:\n",
        "                # Nếu không phát hiện được đối tượng trong frame, sử dụng màu của nhãn trước đó\n",
        "                label_color = previous_label_colors.get(id, (255, 255, 255))\n",
        "\n",
        "            self.labels = {}\n",
        "\n",
        "            # Hiển thị nhãn trên khung hình\n",
        "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 5)\n",
        "            label_x = frame.shape[1] - w - 20\n",
        "            label_y = 50 + h\n",
        "            cv2.rectangle(frame, (label_x, label_y - h - 15), (label_x + w + 10,label_y + 10), (0, 0, 0), -1)\n",
        "            cv2.putText(frame, label, (label_x + 5, label_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.5, label_color, 3)\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def save_first_detected_frame(self, frame, track):\n",
        "        x1, y1, x2, y2 = int(track[0]), int(track[1]), int(track[2]), int(track[3])\n",
        "        id = int(track[4])\n",
        "        class_id = int(track[6])\n",
        "        key = (id, class_id)\n",
        "\n",
        "        if hasattr(self, 'last_saved_key') and self.last_saved_key != key:\n",
        "            # Clear the saved images if there is a change in class or id\n",
        "            self.saved_images.clear()\n",
        "\n",
        "        if key not in self.saved_images:\n",
        "            object_img = frame[y1:y2, x1:x2]\n",
        "            height, width = object_img.shape[:2]\n",
        "            #print(height, width)\n",
        "            if height > 0:\n",
        "              aspect_ratio = width / height\n",
        "              new_width = 300\n",
        "              if aspect_ratio == 0:\n",
        "                new_height = 300\n",
        "              else:\n",
        "                new_height = int(new_width / aspect_ratio)\n",
        "\n",
        "              if new_height > 980:\n",
        "                  new_height = 980\n",
        "                  new_width = int(new_height * aspect_ratio)\n",
        "\n",
        "              resized_img = cv2.resize(object_img, (new_width, new_height))\n",
        "\n",
        "            if height <= 0:\n",
        "              resized_img = cv2.resize(object_img, (300, height))\n",
        "\n",
        "            self.saved_images[key] = resized_img\n",
        "            self.last_saved_key = key\n",
        "\n",
        "    def draw_saved_images(self, frame):\n",
        "        for (id, class_id), img in self.saved_images.items():\n",
        "            x_offset = 1600\n",
        "            y_offset = 100\n",
        "            y_end = y_offset + img.shape[0]\n",
        "            x_end = x_offset + img.shape[1]\n",
        "            cv2.rectangle(frame, (1600, 100), (x_end, 1080), (0, 0, 0), -1)\n",
        "            frame[y_offset:y_end, x_offset:x_end] = img\n",
        "            #print(img.shape)\n",
        "        return frame\n",
        "\n",
        "\n",
        "    def __call__(self):\n",
        "        tracker = self.tracker\n",
        "\n",
        "        # Lấy thông tin từ video kết quả\n",
        "        seq_name = \"StrongSort\"\n",
        "        im_dir = \"img1\"\n",
        "        seq_length = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_rate = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        im_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        im_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        im_ext = \".jpg\"  # Phần mở rộng của ảnh\n",
        "\n",
        "        # Ghi thông tin vào file seqinfo.ini\n",
        "        self.write_seqinfo_ini(seq_name, seq_length, frame_rate, im_width, im_height, im_ext, im_dir)\n",
        "\n",
        "        # Mở file txt để ghi kết quả\n",
        "        with open(\"tracking_result.txt\", \"w\") as txt_file:\n",
        "            txt_file.write(\"timestamp_hms,timestamp_hmsf,frame_idx,fps,object_cls,object_idx,object_id,notes,frame_height,frame_width,scale_height,scale_width,x1,y1,x2,y2,center_x,center_y\\n\")\n",
        "            previous_tracks = []\n",
        "            while True:\n",
        "                start_time = perf_counter()\n",
        "                ret, frame = self.cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                label = \"Unknown\"\n",
        "                (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 5)\n",
        "                label_x = frame.shape[1] - w - 20\n",
        "                label_y = 50 + h\n",
        "                cv2.rectangle(frame, (label_x, label_y - h - 15), (label_x + w + 10, label_y + 10), (0, 0, 0), -1)\n",
        "                cv2.putText(frame, label, (label_x + 5, label_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 3)\n",
        "                detections = self.predict(frame)\n",
        "                for dets in detections:\n",
        "                    tracks = tracker.update(dets.boxes.data.to(\"cpu\").numpy(), frame)\n",
        "                    if len(tracks.shape) == 2 and tracks.shape[1] == 8:\n",
        "                        if len(previous_tracks) > 0:\n",
        "                            tracks = self.update_track_id(tracks, previous_tracks)\n",
        "                        frame = self.draw_tracks(frame, tracks, txt_file)\n",
        "                        previous_tracks = tracks\n",
        "                self.display_labels(frame, tracks)\n",
        "                self.draw_saved_images(frame)\n",
        "                end_time = perf_counter()\n",
        "                # fps = 1 / np.round(end_time - start_time, 2)\n",
        "                # cv2.rectangle(frame, (0, 30), (220, 80), (255, 255, 255), -1)\n",
        "                # cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), self.font, 1.5, (0, 255, 0), 5)\n",
        "                self.writer.write(frame)\n",
        "                #cv2_imshow(frame)\n",
        "                if cv2.waitKey(5) & 0xFF == 27:\n",
        "                    break\n",
        "            self.cap.release()\n",
        "            self.writer.release()\n",
        "            cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9K9M0ZE4ssG"
      },
      "source": [
        "# 3. RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "collapsed": true,
        "id": "mKCTwjpot7IU",
        "outputId": "a1ff2d40-1d67-4288-964f-f94c48d6db3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Device:  cuda:0\n",
            "Model summary (fused): 218 layers, 25841497 parameters, 0 gradients, 78.7 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-06-21 09:12:24.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.utils.torch_utils\u001b[0m:\u001b[36mselect_device\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mYolo Tracking v10.0.72 🚀 Python-3.10.12 torch-2.2.2+cu121\n",
            "CUDA:0 (Tesla T4, 15102MiB)\u001b[0m\n",
            "\u001b[32m2024-06-21 09:12:25.291\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"/content/osnet_x0_25_endocv_30.pt\"\u001b[0m\n"
          ]
        },
        {
          "ename": "error",
          "evalue": "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d2661a561fa8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvideo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tracking_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_video_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3eb9660db561>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_tracks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                             \u001b[0mtracks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_track_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_tracks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_tracks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                         \u001b[0mprevious_tracks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3eb9660db561>\u001b[0m in \u001b[0;36mdraw_tracks\u001b[0;34m(self, frame, tracks, txt_file, overlap_threshold)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_first_detected_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mtxt_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{timestamp_hms},{timestamp_hmsf},{frame_id},{frame_rate},{class_name},{id},{id},{null_notes},{frame.shape[0]},{frame.shape[1]},{scale_height},{scale_width},{x1},{y1},{x2},{y2},{center_x},{center_y}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-3eb9660db561>\u001b[0m in \u001b[0;36msave_first_detected_frame\u001b[0;34m(self, frame, track)\u001b[0m\n\u001b[1;32m    217\u001b[0m                   \u001b[0mnew_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_height\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maspect_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m               \u001b[0mresized_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "detector = ObjectDetection(model_weights, test_vid)\n",
        "detector()\n",
        "video_name = \"tracking_\" + input_video_name + \".mp4\"\n",
        "print(video_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiDJyMPV7rIi"
      },
      "source": [
        "# 4. Generate txt csv results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTneSRIo2UDu"
      },
      "outputs": [],
      "source": [
        "def txt_to_csv(input_txt_file, output_csv_file):\n",
        "    with open(input_txt_file, 'r') as infile, open(output_csv_file, 'w', newline='') as outfile:\n",
        "        reader = csv.reader(infile, delimiter=',')\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        for row in reader:\n",
        "            writer.writerow(row)\n",
        "\n",
        "\n",
        "def convert_file(input_file, output_file):\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        lines = infile.readlines()\n",
        "        for line in lines[1:]:  # Skip the first line (header)\n",
        "            parts = line.strip().split(',')\n",
        "            if len(parts) < 17:\n",
        "                continue  # Skip lines that do not have enough values\n",
        "\n",
        "            frame_id = parts[2]\n",
        "            object_id = parts[5]\n",
        "            x1 = int(parts[12])\n",
        "            y1 = int(parts[13])\n",
        "            x2 = int(parts[14])\n",
        "            y2 = int(parts[15])\n",
        "            conf = round(float(parts[6]), 2)\n",
        "\n",
        "            width = x2 - x1\n",
        "            height = y2 - y1\n",
        "\n",
        "            # Write to the output file\n",
        "            outfile.write(f\"{frame_id},{object_id},{x1},{y1},{width},{height},{conf},-1,-1,-1\\n\")\n",
        "\n",
        "# Usage\n",
        "input_file = 'tracking_result.txt'\n",
        "output_mot_file = 'mot_result.txt'\n",
        "output_csv_file = \"tracking_\" + input_video_name +'.csv'\n",
        "convert_file(input_file, output_mot_file)\n",
        "txt_to_csv(input_file, output_csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEdT88qd2U8N"
      },
      "outputs": [],
      "source": [
        "# Tạo thư mục với tên giống với video_name trong /content/run\n",
        "run_folder = \"/content/runs/htt\"\n",
        "video_name = \"tracking_\" + input_video_name + \".mp4\"\n",
        "video_folder = os.path.join(run_folder, video_name)\n",
        "if not os.path.exists(video_folder):\n",
        "    os.makedirs(video_folder)\n",
        "\n",
        "# Di chuyển video, seqinfo.ini và results.txt vào thư mục vừa tạo\n",
        "os.rename(video_name, os.path.join(video_folder, video_name))\n",
        "os.rename(\"seqinfo.ini\", os.path.join(video_folder, \"seqinfo.ini\"))\n",
        "os.rename(\"mot_result.txt\", os.path.join(video_folder, \"mot_result.txt\"))\n",
        "os.rename(\"tracking_result.txt\", os.path.join(video_folder, \"tracking_result.txt\"))\n",
        "os.rename(output_csv_file, os.path.join(video_folder, output_csv_file))\n",
        "#os.rename('detect_'+input_video_name+'.mp4', os.path.join(video_folder,'detect_'+input_video_name+'.mp4'))\n",
        "print(video_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hexeh0o2Fu08",
        "outputId": "7eed9478-bfb3-435e-d98a-1f8a17a52dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zip file created successfully: /content/run2006.zip\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Define the folder to be zipped and the output zip file path\n",
        "folder_path = '/content/runs'\n",
        "output_zip_path = '/content/run.zip'\n",
        "\n",
        "# Step 3: Zip the folder\n",
        "shutil.make_archive(output_zip_path.replace('.zip', ''), 'zip', folder_path)\n",
        "\n",
        "# Step 4: Verify the zip file is created\n",
        "if os.path.exists(output_zip_path):\n",
        "    print(f'Zip file created successfully: {output_zip_path}')\n",
        "else:\n",
        "    print('Error in creating zip file')\n",
        "\n",
        "# Step 5: Download the zip file\n",
        "#files.download(output_zip_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3rJ6hZGItvC"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/run.zip /content/drive/MyDrive"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2UA2yotWAgjh",
        "8yViuoXlmy9N",
        "ynwB96csJ2tf",
        "cc58-XqGPcD7",
        "fuLvnDk_9rDw",
        "NVXaRpG4nLeX",
        "mIruipeA8jrt",
        "mp-YuAoGXA-X",
        "7sMsJnidXEhq",
        "fiDJyMPV7rIi"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "07a6b268f2658f7bb46e42b9678d82efd26886f1589feca769873f2e2e223d24"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
